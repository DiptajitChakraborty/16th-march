{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f8cfbf-1014-4dd1-86f0-7062f23ee1b4",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea3b6d-a8c3-425a-a26d-bea7ed35f5ad",
   "metadata": {},
   "source": [
    "Ans: When the model is trained with high accuracy but the testerd accuracy of the model is low, then we call it as overfitting.\n",
    " we can mitigate this by early stopping , pruning, regularization, ensembling and data augmentation. \n",
    " \n",
    "and, when te model is trained and tested both with low accuracy, then we call it as underfitting.\n",
    " this can be mitigated by Increase the number of features in the dataset,Increase model complexity,Reduce noise in the data,Increase the duration of training the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731358e3-a783-491c-8d06-ec004df3a08d",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bcca7-87f2-4087-82b3-4bdbfb97b596",
   "metadata": {},
   "source": [
    "Ans:\n",
    "We can prevent overfitting by diversifying and scaling our training data set or using some other data science strategies, like those given below.\n",
    "Early stopping\n",
    "Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.\n",
    "Pruning\n",
    "we might identify several features or parameters that impact the final prediction when we build a model. Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones. For example, to predict if an image is an animal or human, we can look at various input parameters like face shape, ear position, body structure, etc. We may prioritize face shape and ignore the shape of the eyes.\n",
    "Regularization\n",
    "Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. For example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\n",
    "Ensembling\n",
    "Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while bagging trains them in parallel.\n",
    "Data augmentation\n",
    "Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. We can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics. For example, applying transformations such as translation, flipping, and rotation to input images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940eee2-ef6e-4d7e-87b7-8b11e6079104",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084e1a6-d33f-4885-9824-85dca1bd3c06",
   "metadata": {},
   "source": [
    "Ans: when te model is trained and tested both with low accuracy, then we call it as underfitting.\n",
    "\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.An underfitted model has high bias and low variance.Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618912c-7e4d-4a8a-ae23-51c7e9413b93",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edced10-af24-4b1a-86f5-05da9b61413b",
   "metadata": {},
   "source": [
    "Ans:The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as Underfitting of Data. This happens when the hypothesis is too simple or linear in nature.\n",
    "The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high.While training a data model variance should be kept low.\n",
    "\n",
    "Bias and variance are inversely connected.For an accurate prediction of the model, algorithms need a low variance and low bias. But this is not possible because bias and variance are related to each other: If we decrease the variance, it will increase the bias. If we decrease the bias, it will increase the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8755ae-505c-4d0e-a131-4597e995f00e",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85fd53-2483-45cb-aff8-d16df159139d",
   "metadata": {},
   "source": [
    "Ans:We can identify overfitting by looking at validation metrics, like loss or accuracy. Usually, the validation metric stops improving after a certain number of epochs and begins to decrease afterward. The training metric continues to improve because the model seeks to find the best fit for the training data.We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. We model is underfitting the training data when the model performs poorly on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e950e6-b832-406b-bbc8-0408a91253b3",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af5b06-ae80-465b-96dc-38cbddf1c534",
   "metadata": {},
   "source": [
    "Ans: When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations.\n",
    "The disparity between the values that were predicted and the values that were actually observed is referred to as bias\n",
    "The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data.\n",
    "\n",
    "The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation.\n",
    "\n",
    "\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.linear discriminant analysis and logistic regression can be used to assess the same research problems. Their functional form is the same but they differ in the method of the estimation of their coefficient. Discriminant analysis produces a score, similar to the production of logit of the logistic regression. Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67278f-f387-478f-80b8-216b85508e41",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2807e34-fdd1-4e02-b8a2-fd83b7754580",
   "metadata": {},
   "source": [
    "Ans: Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "There are three main regularization techniques, namely: Ridge Regression (L2 Norm) Lasso (L1 Norm) Dropout.\n",
    "\n",
    "Ridge regression is also called L2 norm or regularization.\n",
    "When using this technique, we add the sum of weight’s square to a loss function and thus create a new loss function.\n",
    "Lasso (L1 Norm) is different from ridge regression as it uses absolute weight values for normalization.\n",
    "Dropout is a regularization technique used in neural networks. It prevents complex co-adaptations from other neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa0fd2-125e-4a64-a0a5-571ed7495a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ec77d-cf56-4232-9a30-4fddbb26b0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688f6ad-448c-4302-ab23-f8f2b2e39a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed31cd1-01e6-4b4c-9c6f-7db2ea4f9b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553df453-3d1d-4382-8425-953ec833c388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c7944-6962-4835-b3f0-f7103c2c2e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d1275-f401-492a-a384-834ce5357eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
